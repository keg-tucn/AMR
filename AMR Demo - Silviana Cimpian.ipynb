{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/silvianacmp/PycharmProjects/ActionGeneration/plots_keras\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from AMRGraph import AMR\n",
    "from AMRData import CustomizedAMR\n",
    "from utilities import pretty_print, generate_action_sequence, generate_custom_amr\n",
    "import preprocessing.ActionSequenceGenerator as asc\n",
    "from preprocessing.DependencyExtractor import extract_dependencies\n",
    "from preprocessing import TokensReplacer\n",
    "from keras_lstm_flow import test, test_file\n",
    "from postprocessing import ActionSequenceReconstruction as asr\n",
    "from smatch import smatch_amr\n",
    "from smatch import smatch_util\n",
    "from deep_dynet import support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model1 = \"proxy_epochs=40_maxlen=20_embeddingsdim=300\"\n",
    "max_len1=20\n",
    "embeddings_dim1=300\n",
    "\n",
    "model2 = \"all_epochs=15_maxlen=30_embeddingsdim=300\"\n",
    "max_len2=30\n",
    "embeddings_dim2=300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This demo presents the pipeline flow for parsing a sentence into Abstract Meaning Representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We begin by looking at a sentence and its human-annotated Abstract Meaning Representation graph:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/example1.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence = \"and there is precious little room for interpretation .\"\n",
    "amr_str = \"\"\"(a / and~e.0\n",
    "      :op2 (r / room~e.5\n",
    "            :quant (l / little~e.4)\n",
    "            :mod (p / precious~e.3)\n",
    "            :purpose~e.6 (i / interpret-01~e.7)))\"\"\"\n",
    "amr = AMR.parse_string(amr_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mappings between node variables and their corresponding concepts.\n",
      "\n",
      "{'i': 'interpret-01', 'p': 'precious', 'r': 'room', 'l': 'little', 'a': 'and'}\n",
      "\n",
      "Mappings between nodes and all the aligned tokens: If the nodes don't havea variable (polarity, literals, quantities, interrogatives), they specify both the aligned tokens and the parent in order to uniquely identify them\n",
      "\n",
      "{'i': ['7'], 'p': ['3'], 'r': ['5'], 'l': ['4'], 'a': ['0']}\n",
      "\n",
      "Mappings between relations and tokens. Uniquely identified by also specifying the parent of that relation.\n",
      "\n",
      "{'purpose': [[('6', 'r')]]}\n",
      "\n",
      "Mappings from a node to each child, along with the relation between them.\n",
      "\n",
      "Key: i\n",
      "Leaf\n",
      "\n",
      "Key: p\n",
      "Leaf\n",
      "\n",
      "Key: r\n",
      "quant -> l\n",
      "purpose -> i\n",
      "mod -> p\n",
      "\n",
      "Key: l\n",
      "Leaf\n",
      "\n",
      "Key: a\n",
      "op2 -> r\n",
      "\n",
      "\n",
      "All the nodes in the amr should appear here.\n",
      "\n",
      "['i', 'p', 'r', 'l', 'a']\n",
      "\n",
      "Creating custom AMR.\n",
      "\n",
      "\n",
      "Custom AMR token to concepts dict\n",
      "\n",
      "{0: ('a', 'and'), 3: ('p', 'precious'), 4: ('l', 'little'), 5: ('r', 'room'), 7: ('i', 'interpret-01')}\n",
      "\n",
      "Custom AMR relations dict\n",
      "\n",
      "{('p', 'r'): ('mod', [], ['3']), ('r', 'a'): ('op2', ['l', 'i', 'p'], ['5']), ('i', 'r'): ('purpose', [], ['7']), ('a', ''): ('', ['r'], ['0']), ('l', 'r'): ('quant', [], ['4'])}\n",
      "\n",
      "Custom AMR parent dict\n",
      "\n",
      "{'i': 'r', 'p': 'r', 'r': 'a', 'l': 'r', 'a': ''}\n"
     ]
    }
   ],
   "source": [
    "custom_amr = generate_custom_amr(amr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Having the sentence and the CustomAMR structure, we can now generate the <span style=\"color:red\">*oracle action sequence*</span>\n",
    "\"and there is precious little room for interpretation .\"\n",
    "\n",
    "```(a / and~e.0\n",
    "      :op2 (r / room~e.5\n",
    "            :quant (l / little~e.4)\n",
    "            :mod (p / precious~e.3)\n",
    "            :purpose~e.6 (i / interpret-01~e.7)))```\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "actions = asc.generate_action_sequence(custom_amr, sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SH_and_a',\n",
       " 'DN',\n",
       " 'DN',\n",
       " 'SH_precious_p',\n",
       " 'SH_little_l',\n",
       " 'SH_room_r',\n",
       " 'RL_quant',\n",
       " 'RL_mod',\n",
       " 'DN',\n",
       " 'SH_interpret-01_i',\n",
       " 'RR_purpose',\n",
       " 'RR_op2',\n",
       " 'DN']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We next extract the <span style=\"color:red\">dependencies</span> between the tokens in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "deps = extract_dependencies(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And(0) there(1) is(2) precious(3) little(4) room(5) for(5) interpretation(6) .(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: (5, 'nsubj'),\n",
       " 1: (5, 'advmod'),\n",
       " 2: (5, 'cop'),\n",
       " 3: (5, 'amod'),\n",
       " 4: (5, 'amod'),\n",
       " 7: (5, 'prep_for')}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We now have all the data necessary for making the prediction using the <span style=\"color:red\">single LSTM model</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model path is:\n",
      "./models/all_epochs=15_maxlen=30_embeddingsdim=300\n",
      "Word index len: \n",
      "7107\n",
      "Test data shape: \n",
      "(1, 9)\n",
      "(1, 13)\n",
      "(1,)\n",
      "1\n",
      "Found 400000 word vectors.\n",
      "Embedding match for volume-quantity\n",
      "Embedding match for distance-quantity\n",
      "Embedding match for energy-quantity\n",
      "Embedding match for power-quantity\n",
      "Embedding match for mass-quantity\n",
      "Embedding match for monetary-quantity\n",
      "Embedding match for temporal-quantity\n",
      "Embedding match for date-entity\n",
      "First 2 not found: [\"don'cha\", 'it...']\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 30)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_2 (InputLayer)             (None, 30)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_3 (InputLayer)             (None, 30)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_4 (InputLayer)             (None, 30)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)          (None, 30, 300)       2132700                                      \n",
      "____________________________________________________________________________________________________\n",
      "input_5 (InputLayer)             (None, 30, 5)         0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_6 (InputLayer)             (None, 30, 6)         0                                            \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, 30, 1211)      0                                            \n",
      "____________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                    (None, 30, 1024)      9158656                                      \n",
      "____________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistribu (None, 30, 5)         5125                                         \n",
      "====================================================================================================\n",
      "Total params: 11,296,481.0\n",
      "Trainable params: 9,163,781.0\n",
      "Non-trainable params: 2,132,700.0\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 30)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_2 (InputLayer)             (None, 30)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_3 (InputLayer)             (None, 30)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_4 (InputLayer)             (None, 30)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)          (None, 30, 300)       2132700                                      \n",
      "____________________________________________________________________________________________________\n",
      "input_5 (InputLayer)             (None, 30, 5)         0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_6 (InputLayer)             (None, 30, 6)         0                                            \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, 30, 1211)      0                                            \n",
      "____________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                    (None, 30, 1024)      9158656                                      \n",
      "____________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistribu (None, 30, 5)         5125                                         \n",
      "====================================================================================================\n",
      "Total params: 11,296,481.0\n",
      "Trainable params: 9,163,781.0\n",
      "Non-trainable params: 2,132,700.0\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "Buffer and stack at end of prediction\n",
      "[]\n",
      "[16]\n",
      "Sentence\n",
      "and there is precious little room for interpretation . \n",
      "\n",
      "Predicted\n",
      "SH DN DN SH SH RL SH RL DN SH RR DN RR \n",
      "\n",
      "Actual\n",
      "SH DN DN SH SH SH RL RL DN SH RR RR DN \n",
      "\n",
      "Predictions with old labels: \n",
      "['SH_and', 'DN', 'DN', 'SH_precious', 'SH_little', 'RL_quant', 'SH_room', 'RL_mod', 'DN', 'SH_interpret-01', 'RR_purpose', 'DN', 'RR_op2']\n",
      "Original Amr\n",
      "(a / and~e.0\n",
      "      :op2 (r / room~e.5\n",
      "            :quant (l / little~e.4)\n",
      "            :mod (p / precious~e.3)\n",
      "            :purpose~e.6 (i / interpret-01~e.7)))\n",
      "Predicted Amr\n",
      "( d1 / and \n",
      "\t:op2  ( d1_1 / room \n",
      "\t\t:mod  ( d1_1_1 / little \n",
      "\t\t\t:quant  ( d1_1_1_1 / precious )\n",
      "\t\t)\n",
      "\t\t:purpose  ( d1_1_2 / interpret-01 )\n",
      "\t)\n",
      ")\n",
      "Smatch f-score 0.800000\n",
      "1/1 [==============================] - 0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0, 3, 3, 0, 0, 1, 0, 1, 3, 0, 2, 3, 2]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(model_name=model2, \n",
    "     tokenizer_path=\"./tokenizers/full_tokenizer.dump\",\n",
    "     test_case_name=\"test1\",\n",
    "     data=[(sentence, actions, amr_str, deps, [], [])], \n",
    "     max_len=max_len2, \n",
    "     embedding_dim=embeddings_dim1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/example1_pred.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We now look at a more complex example, which also contains a <span style=\"color:red\">Named-Entity</span>.  Named entities are identified and replaced at preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/demo_ne_example.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence = \"upgrade fire control systems of Indian tanks .\"\n",
    "\n",
    "amr_str= \"\"\"(u / upgrade-02~e.0 \n",
    "      :ARG1 (s / system~e.3 \n",
    "            :ARG0-of (c / control-01~e.2 \n",
    "                  :ARG1 (f / fire-01~e.1)) \n",
    "            :poss~e.4 (t / tank~e.6 \n",
    "                  :mod (c2 / country :wiki \"India\" \n",
    "                        :name (n / name :op1 \"India\"~e.5)))))\"\"\"\n",
    "amr = AMR.parse_string(amr_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We replace the named-entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "concepts_metadata = {}\n",
    "(new_amr, new_sentence, named_entities) = TokensReplacer.replace_named_entities(amr, sentence)\n",
    "for name_entity in named_entities:\n",
    "    concepts_metadata[name_entity[0]] = name_entity[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'upgrade fire control systems of country tanks .'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AMR(util.ListMap,\n",
       "    {u'India': ListMap(list, {}),\n",
       "     'c': ListMap(list, {'ARG1': [('f',)]}),\n",
       "     'c2': ListMap(list, {'name': [('n',)], 'wiki': [(u'India',)]}),\n",
       "     'f': ListMap(list, {}),\n",
       "     'n': ListMap(list, {'op1': [(u'India',)]}),\n",
       "     's': ListMap(list, {'ARG0-of': [('c',)], 'poss': [('t',)]}),\n",
       "     't': ListMap(list, {'mod': [('c2',)]}),\n",
       "     'u': ListMap(list, {'ARG1': [('s',)]})})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('c2',\n",
       "  'n',\n",
       "  [u'India'],\n",
       "  5,\n",
       "  5,\n",
       "  <amr_util.Node.Node instance at 0x7f4831af8050>)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "named_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AMR(util.ListMap,\n",
       "    {'c': ListMap(list, {'ARG1': [('f',)]}),\n",
       "     'c2': {},\n",
       "     'f': ListMap(list, {}),\n",
       "     's': ListMap(list, {'ARG0-of': [('c',)], 'poss': [('t',)]}),\n",
       "     't': ListMap(list, {'mod': [('c2',)]}),\n",
       "     'u': ListMap(list, {'ARG1': [('s',)]})})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_amr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "( d1 / country \n",
      "\t:name  ( d1_1 / name \n",
      "\t\t:op1 \"\"India\"\"\n",
      "\t)\n",
      "\t:wiki \"\"India\"\"\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print concepts_metadata['c2'].amr_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We now generate the action sequence for the preprocessed AMR graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mappings between node variables and their corresponding concepts.\n",
      "\n",
      "{'c': 'control-01', 'f': 'fire-01', 's': 'system', 'u': 'upgrade-02', 't': 'tank', 'c2': 'country'}\n",
      "\n",
      "Mappings between nodes and all the aligned tokens: If the nodes don't havea variable (polarity, literals, quantities, interrogatives), they specify both the aligned tokens and the parent in order to uniquely identify them\n",
      "\n",
      "{'c': ['2'], 'f': ['1'], 's': ['3'], 'u': ['0'], 't': ['6'], 'c2': [5]}\n",
      "\n",
      "Mappings between relations and tokens. Uniquely identified by also specifying the parent of that relation.\n",
      "\n",
      "{'poss': [[('4', 's')]]}\n",
      "\n",
      "Mappings from a node to each child, along with the relation between them.\n",
      "\n",
      "Key: c\n",
      "ARG1 -> f\n",
      "\n",
      "Key: f\n",
      "Leaf\n",
      "\n",
      "Key: s\n",
      "ARG0-of -> c\n",
      "poss -> t\n",
      "\n",
      "Key: u\n",
      "ARG1 -> s\n",
      "\n",
      "Key: t\n",
      "mod -> c2\n",
      "\n",
      "Key: c2\n",
      "Leaf\n",
      "\n",
      "\n",
      "All the nodes in the amr should appear here.\n",
      "\n",
      "['c', 'f', 's', 'u', 't', 'c2']\n",
      "\n",
      "Creating custom AMR.\n",
      "\n",
      "\n",
      "Custom AMR token to concepts dict\n",
      "\n",
      "{0: ('u', 'upgrade-02'), 1: ('f', 'fire-01'), 2: ('c', 'control-01'), 3: ('s', 'system'), 5: ('c2', 'country'), 6: ('t', 'tank')}\n",
      "\n",
      "Custom AMR relations dict\n",
      "\n",
      "{('c', 's'): ('ARG0-of', ['f'], ['2']), ('u', ''): ('', ['s'], ['0']), ('c2', 't'): ('mod', [], [5]), ('t', 's'): ('poss', ['c2'], ['6']), ('s', 'u'): ('ARG1', ['c', 't'], ['3']), ('f', 'c'): ('ARG1', [], ['1'])}\n",
      "\n",
      "Custom AMR parent dict\n",
      "\n",
      "{'c': 's', 'f': 'c', 's': 'u', 'u': '', 't': 's', 'c2': 't'}\n"
     ]
    }
   ],
   "source": [
    "custom_amr = generate_custom_amr(new_amr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "actions = asc.generate_action_sequence(custom_amr, new_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SH_upgrade-02_u',\n",
       " 'SH_fire-01_f',\n",
       " 'SH_control-01_c',\n",
       " 'RL_ARG1',\n",
       " 'SH_system_s',\n",
       " 'RL_ARG0-of',\n",
       " 'DN',\n",
       " 'SH_country_c2',\n",
       " 'SH_tank_t',\n",
       " 'RL_mod',\n",
       " 'RR_poss',\n",
       " 'RR_ARG1',\n",
       " 'DN']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "deps = extract_dependencies(new_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: (3, 'nn'), 2: (3, 'nn'), 3: (0, 'dobj'), 5: (6, 'nn'), 6: (3, 'prep_of')}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model path is:\n",
      "./models/proxy_epochs=40_maxlen=20_embeddingsdim=300\n",
      "Word index len: \n",
      "7107\n",
      "Test data shape: \n",
      "(1, 8)\n",
      "(1, 13)\n",
      "(1,)\n",
      "1\n",
      "Found 400000 word vectors.\n",
      "Embedding match for volume-quantity\n",
      "Embedding match for distance-quantity\n",
      "Embedding match for energy-quantity\n",
      "Embedding match for power-quantity\n",
      "Embedding match for mass-quantity\n",
      "Embedding match for monetary-quantity\n",
      "Embedding match for temporal-quantity\n",
      "Embedding match for date-entity\n",
      "First 2 not found: [\"don'cha\", 'it...']\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_7 (InputLayer)             (None, 20)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_8 (InputLayer)             (None, 20)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_9 (InputLayer)             (None, 20)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_10 (InputLayer)            (None, 20)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)          (None, 20, 300)       2132700                                      \n",
      "____________________________________________________________________________________________________\n",
      "input_11 (InputLayer)            (None, 20, 5)         0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_12 (InputLayer)            (None, 20, 6)         0                                            \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)      (None, 20, 1211)      0                                            \n",
      "____________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                    (None, 20, 1024)      9158656                                      \n",
      "____________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistribu (None, 20, 5)         5125                                         \n",
      "====================================================================================================\n",
      "Total params: 11,296,481.0\n",
      "Trainable params: 9,163,781.0\n",
      "Non-trainable params: 2,132,700.0\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_7 (InputLayer)             (None, 20)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_8 (InputLayer)             (None, 20)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_9 (InputLayer)             (None, 20)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_10 (InputLayer)            (None, 20)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)          (None, 20, 300)       2132700                                      \n",
      "____________________________________________________________________________________________________\n",
      "input_11 (InputLayer)            (None, 20, 5)         0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_12 (InputLayer)            (None, 20, 6)         0                                            \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)      (None, 20, 1211)      0                                            \n",
      "____________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                    (None, 20, 1024)      9158656                                      \n",
      "____________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistribu (None, 20, 5)         5125                                         \n",
      "====================================================================================================\n",
      "Total params: 11,296,481.0\n",
      "Trainable params: 9,163,781.0\n",
      "Non-trainable params: 2,132,700.0\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "Buffer and stack at end of prediction\n",
      "[]\n",
      "[2656]\n",
      "Sentence\n",
      "upgrade fire control systems of country tanks . \n",
      "\n",
      "Predicted\n",
      "SH SH SH RL SH RL DN SH SH RL RR DN RR \n",
      "\n",
      "Actual\n",
      "SH SH SH RL SH RL DN SH SH RL RR RR DN \n",
      "\n",
      "Predictions with old labels: \n",
      "['SH_upgrade-02', 'SH_fire-01', 'SH_control-01', 'RL_ARG1', 'SH_system', 'RL_ARG0-of', 'DN', 'SH_country', 'SH_tank', 'RL_mod', 'RR_poss', 'DN', 'RR_ARG1']\n",
      "Original Amr\n",
      "(u / upgrade-02~e.0 \n",
      "      :ARG1 (s / system~e.3 \n",
      "            :ARG0-of (c / control-01~e.2 \n",
      "                  :ARG1 (f / fire-01~e.1)) \n",
      "            :poss~e.4 (t / tank~e.6 \n",
      "                  :mod (c2 / country :wiki \"India\" \n",
      "                        :name (n / name :op1 \"India\"~e.5)))))\n",
      "Predicted Amr\n",
      "( d1 / upgrade-02 \n",
      "\t:ARG1  ( d1_1 / system \n",
      "\t\t:ARG0-of  ( d1_1_1 / control-01 \n",
      "\t\t\t:ARG1  ( d1_1_1_1 / fire-01 )\n",
      "\t\t)\n",
      "\t\t:poss  ( d1_1_2 / tank \n",
      "\t\t\t:mod  ( d1_1_2_1 / country )\n",
      "\t\t)\n",
      "\t)\n",
      ")\n",
      "Smatch f-score 0.857143\n",
      "1/1 [==============================] - 0s\n"
     ]
    }
   ],
   "source": [
    "predictions = test(model_name=model1, \n",
    "                   tokenizer_path=\"./tokenizers/full_tokenizer.dump\",\n",
    "                   test_case_name=\"test2\",\n",
    "                   data=[(new_sentence, actions, amr_str, deps, [], [])], \n",
    "                   max_len=max_len1, \n",
    "                   embedding_dim=embeddings_dim1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our score is penalized by the fact that the whole Named-Entity subtree is pruned. We now show how the subtree is recovered based on the named entity metadata. From the named entities meta-data we extract the list of literals (\"India\") and the beginning index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/example2_pred.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "named_entities = [(n[3], n[2]) for n in named_entities]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5, [u'India'])]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "named_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 1, 0, 1, 3, 0, 0, 1, 2, 3, 2]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions with old labels: \n",
      "['SH_upgrade-02', 'SH_fire-01', 'SH_control-01', 'RL_ARG1', 'SH_system', 'RL_ARG0-of', 'DN', 'SH_country', 'SH_tank', 'RL_mod', 'RR_poss', 'DN', 'RR_ARG1']\n",
      "Original Amr\n",
      "(u / upgrade-02~e.0 \n",
      "      :ARG1 (s / system~e.3 \n",
      "            :ARG0-of (c / control-01~e.2 \n",
      "                  :ARG1 (f / fire-01~e.1)) \n",
      "            :poss~e.4 (t / tank~e.6 \n",
      "                  :mod (c2 / country :wiki \"India\" \n",
      "                        :name (n / name :op1 \"India\"~e.5)))))\n",
      "Predicted Amr\n",
      "( d1 / upgrade-02 \n",
      "\t:ARG1  ( d1_1 / system \n",
      "\t\t:ARG0-of  ( d1_1_1 / control-01 \n",
      "\t\t\t:ARG1  ( d1_1_1_1 / fire-01 )\n",
      "\t\t)\n",
      "\t\t:poss  ( d1_1_2 / tank \n",
      "\t\t\t:mod  ( d1_1_2_1 / country \n",
      "\t\t\t\t:wiki \"India\"\n",
      "\t\t\t\t:name  ( d1_1_2_1_1 / name \n",
      "\t\t\t\t\t:op1 \"India\"\n",
      "\t\t\t\t)\n",
      "\t\t\t)\n",
      "\t\t)\n",
      "\t)\n",
      ")\n",
      "Smatch f-score 1.000000\n"
     ]
    }
   ],
   "source": [
    "vocab_acts = support.Vocab.from_list(['SH', 'RL', 'RR', 'DN', 'SW'])\n",
    "action_objects = support.oracle_actions_to_action_index(actions, vocab_acts)\n",
    "action_indices = [a.index for a in action_objects]\n",
    "action_labels = [a.label for a in action_objects]\n",
    "\n",
    "act = asr.ActionConceptTransfer()\n",
    "act.load_from_action_indices_and_labels(action_indices, action_labels)\n",
    "pred_label = act.populate_new_actions(predictions[0])\n",
    "print 'Predictions with old labels: '\n",
    "print pred_label\n",
    "predicted_amr_str = asr.reconstruct_all_ne(pred_label, named_entities, [])\n",
    "\n",
    "\n",
    "smatch_results = smatch_util.SmatchAccumulator()\n",
    "original_amr = smatch_amr.AMR.parse_AMR_line(amr_str)\n",
    "predicted_amr = smatch_amr.AMR.parse_AMR_line(predicted_amr_str)\n",
    "smatch_f_score = smatch_results.compute_and_add(predicted_amr, original_amr)\n",
    "\n",
    "print 'Original Amr'\n",
    "print amr_str\n",
    "print 'Predicted Amr'\n",
    "print predicted_amr_str\n",
    "print 'Smatch f-score %f' % smatch_f_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A sentence with Swap. Swap is hard to predict because it appears very rarely in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/example3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "['DN', 'SH_seem-01_s2', 'DN', 'DN', 'DN', 'SH_future_f', 'DN', 'SH_we_w', 'SH_have-03_h', 'RL_ARG0', 'DN', 'SH_scheme_s', 'DN', 'DN', 'SH_major-02_m', 'SH_renovate-01_r', 'SH_plan-01_p', 'RL_ARG1', 'RL_ARG1-of', 'RR_mod', 'DN', 'SW', 'RL_purpose', 'RR_ARG1', 'RR_ARG1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mappings between node variables and their corresponding concepts.\n",
      "\n",
      "{'f': 'future', 'h': 'have-03', 'm': 'major-02', 'p': 'plan-01', 's': 'scheme', 'r': 'renovate-01', 'w': 'we', 's2': 'seem-01'}\n",
      "\n",
      "Mappings between nodes and all the aligned tokens: If the nodes don't havea variable (polarity, literals, quantities, interrogatives), they specify both the aligned tokens and the parent in order to uniquely identify them\n",
      "\n",
      "{'f': ['5'], 'h': ['8'], 'm': ['13'], 'p': ['15'], 's': ['10'], 'r': ['14'], 'w': ['7'], 's2': ['1']}\n",
      "\n",
      "Mappings between relations and tokens. Uniquely identified by also specifying the parent of that relation.\n",
      "\n",
      "{'ARG1': [[('3', 's2')]]}\n",
      "\n",
      "Mappings from a node to each child, along with the relation between them.\n",
      "\n",
      "Key: f\n",
      "Leaf\n",
      "\n",
      "Key: h\n",
      "ARG0 -> w\n",
      "ARG1 -> s\n",
      "\n",
      "Key: m\n",
      "Leaf\n",
      "\n",
      "Key: p\n",
      "ARG1 -> r\n",
      "ARG1-of -> m\n",
      "\n",
      "Key: s\n",
      "purpose -> f\n",
      "mod -> p\n",
      "\n",
      "Key: r\n",
      "Leaf\n",
      "\n",
      "Key: w\n",
      "Leaf\n",
      "\n",
      "Key: s2\n",
      "ARG1 -> h\n",
      "\n",
      "\n",
      "All the nodes in the amr should appear here.\n",
      "\n",
      "['f', 'h', 'm', 'p', 's', 'r', 'w', 's2']\n",
      "\n",
      "Creating custom AMR.\n",
      "\n",
      "\n",
      "Custom AMR token to concepts dict\n",
      "\n",
      "{1: ('s2', 'seem-01'), 5: ('f', 'future'), 7: ('w', 'we'), 8: ('h', 'have-03'), 10: ('s', 'scheme'), 13: ('m', 'major-02'), 14: ('r', 'renovate-01'), 15: ('p', 'plan-01')}\n",
      "\n",
      "Custom AMR relations dict\n",
      "\n",
      "{('p', 's'): ('mod', ['r', 'm'], ['15']), ('r', 'p'): ('ARG1', [], ['14']), ('s2', ''): ('', ['h'], ['1']), ('f', 's'): ('purpose', [], ['5']), ('h', 's2'): ('ARG1', ['w', 's'], ['8']), ('w', 'h'): ('ARG0', [], ['7']), ('m', 'p'): ('ARG1-of', [], ['13']), ('s', 'h'): ('ARG1', ['f', 'p'], ['10'])}\n",
      "\n",
      "Custom AMR parent dict\n",
      "\n",
      "{'f': 's', 'h': 's2', 'm': 'p', 'p': 's', 's': 'h', 'r': 'p', 'w': 'h', 's2': ''}\n",
      "['DN', 'SH_seem-01_s2', 'DN', 'DN', 'DN', 'SH_future_f', 'DN', 'SH_we_w', 'SH_have-03_h', 'RL_ARG0', 'DN', 'SH_scheme_s', 'DN', 'DN', 'SH_major-02_m', 'SH_renovate-01_r', 'SH_plan-01_p', 'RL_ARG1', 'RL_ARG1-of', 'RR_mod', 'DN', 'SW', 'RL_purpose', 'RR_ARG1', 'RR_ARG1']\n",
      "Model path is:\n",
      "./models/all_epochs=15_maxlen=30_embeddingsdim=300\n",
      "Word index len: \n",
      "7107\n",
      "Test data shape: \n",
      "(1, 17)\n",
      "(1, 25)\n",
      "(1,)\n",
      "1\n",
      "Found 400000 word vectors.\n",
      "Embedding match for volume-quantity\n",
      "Embedding match for distance-quantity\n",
      "Embedding match for energy-quantity\n",
      "Embedding match for power-quantity\n",
      "Embedding match for mass-quantity\n",
      "Embedding match for monetary-quantity\n",
      "Embedding match for temporal-quantity\n",
      "Embedding match for date-entity\n",
      "First 2 not found: [\"don'cha\", 'it...']\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_13 (InputLayer)            (None, 30)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_14 (InputLayer)            (None, 30)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_15 (InputLayer)            (None, 30)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_16 (InputLayer)            (None, 30)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)          (None, 30, 300)       2132700                                      \n",
      "____________________________________________________________________________________________________\n",
      "input_17 (InputLayer)            (None, 30, 5)         0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_18 (InputLayer)            (None, 30, 6)         0                                            \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)      (None, 30, 1211)      0                                            \n",
      "____________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                    (None, 30, 1024)      9158656                                      \n",
      "____________________________________________________________________________________________________\n",
      "time_distributed_3 (TimeDistribu (None, 30, 5)         5125                                         \n",
      "====================================================================================================\n",
      "Total params: 11,296,481.0\n",
      "Trainable params: 9,163,781.0\n",
      "Non-trainable params: 2,132,700.0\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_13 (InputLayer)            (None, 30)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_14 (InputLayer)            (None, 30)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_15 (InputLayer)            (None, 30)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_16 (InputLayer)            (None, 30)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)          (None, 30, 300)       2132700                                      \n",
      "____________________________________________________________________________________________________\n",
      "input_17 (InputLayer)            (None, 30, 5)         0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_18 (InputLayer)            (None, 30, 6)         0                                            \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)      (None, 30, 1211)      0                                            \n",
      "____________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                    (None, 30, 1024)      9158656                                      \n",
      "____________________________________________________________________________________________________\n",
      "time_distributed_3 (TimeDistribu (None, 30, 5)         5125                                         \n",
      "====================================================================================================\n",
      "Total params: 11,296,481.0\n",
      "Trainable params: 9,163,781.0\n",
      "Non-trainable params: 2,132,700.0\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "Buffer and stack at end of prediction\n",
      "[]\n",
      "[14]\n",
      "Sentence\n",
      "it seems that for the future , we have a scheme , a major renovation plan . \n",
      "\n",
      "Predicted\n",
      "SH SH SH DN DN SH RL DN SH SH RL DN SH RL DN DN SH SH RL RL RL SH RL RR DN RR \n",
      "\n",
      "Actual\n",
      "DN SH DN DN DN SH DN SH SH RL DN SH DN DN SH SH SH RL RL RR DN SW RL RR RR \n",
      "\n",
      "Predictions with old labels: \n",
      "['SH_seem-01', 'SH_future', 'SH_we', 'DN', 'DN', 'SH_have-03', 'RL_ARG0', 'DN', 'SH_scheme', 'SH_major-02', 'RL_ARG1', 'DN', 'SH_renovate-01', 'RL_ARG1-of', 'DN', 'DN', 'SH_plan-01', 'SH_unk', 'RL_mod', 'RL_purpose', 'RL_ARG1', 'SH_unk', 'RL_ARG1', 'RR_unk', 'DN', 'RR_unk']\n",
      "Original Amr\n",
      "(s2 / seem-01~e.1\n",
      "       :ARG1~e.3 (h / have-03~e.8\n",
      "             :ARG0 (w / we~e.7)\n",
      "             :ARG1 (s / scheme~e.10\n",
      "                   :mod (p / plan-01~e.15\n",
      "                   :ARG1 (r / renovate-01~e.14)\n",
      "                         :ARG1-of (m / major-02~e.13))\n",
      "                   :purpose (f / future~e.5))))\n",
      "Predicted Amr\n",
      "( d1 / seem-01 \n",
      "\t:unk  ( d1_1 / future \n",
      "\t\t:unk  ( d1_1_1 / unk \n",
      "\t\t\t:ARG1  ( d1_1_1_1 / unk \n",
      "\t\t\t\t:mod  ( d1_1_1_1_1 / plan-01 )\n",
      "\t\t\t\t:purpose  ( d1_1_1_1_2 / renovate-01 \n",
      "\t\t\t\t\t:ARG1-of  ( d1_1_1_1_2_1 / major-02 \n",
      "\t\t\t\t\t\t:ARG1  ( d1_1_1_1_2_1_1 / scheme )\n",
      "\t\t\t\t\t)\n",
      "\t\t\t\t)\n",
      "\t\t\t\t:ARG1  ( d1_1_1_1_3 / have-03 \n",
      "\t\t\t\t\t:ARG0  ( d1_1_1_1_3_1 / we )\n",
      "\t\t\t\t)\n",
      "\t\t\t)\n",
      "\t\t)\n",
      "\t)\n",
      ")\n",
      "Smatch f-score 0.555556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s\n"
     ]
    }
   ],
   "source": [
    "amr_str = \"\"\"(s2 / seem-01~e.1\n",
    "       :ARG1~e.3 (h / have-03~e.8\n",
    "             :ARG0 (w / we~e.7)\n",
    "             :ARG1 (s / scheme~e.10\n",
    "                   :mod (p / plan-01~e.15\n",
    "                   :ARG1 (r / renovate-01~e.14)\n",
    "                         :ARG1-of (m / major-02~e.13))\n",
    "                   :purpose (f / future~e.5))))\"\"\"\n",
    "sentence = \"\"\"It seems that for the future , we have a scheme , a major renovation plan .\"\"\"\n",
    "amr = AMR.parse_string(amr_str)\n",
    "custom_amr = generate_custom_amr(amr)\n",
    "actions = generate_action_sequence(custom_amr, sentence)\n",
    "print actions\n",
    "deps = extract_dependencies(sentence)\n",
    "predictions = test(model_name=model2, tokenizer_path=\"./tokenizers/full_tokenizer.dump\", test_case_name=\"test3\",\n",
    "     data=[(sentence, actions, amr_str, deps, [], [])], max_len=max_len2, embedding_dim=embeddings_dim2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/example3_pred.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### We will now test a sentence with a <span style=\"color:red\">date-entity</span> in it. We preprocess the date-entity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/example4.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "amr_str = \"\"\"(d / difficult~e.5 \n",
    "      :domain~e.4 (r / reach-01~e.7 \n",
    "            :ARG1 (c / consensus~e.0 \n",
    "                  :topic~e.1 (c2 / country :wiki \"India\" \n",
    "                        :name (n / name :op1 \"India\"~e.2))) \n",
    "            :time~e.8 (m / meet-03~e.11 \n",
    "                  :ARG0 (o / organization :wiki \"Nuclear_Suppliers_Group\" \n",
    "                        :name (n2 / name :op1 \"NSG\"~e.10)) \n",
    "                  :time~e.12 (d2 / date-entity :year 2007~e.14 :month~e.13 11~e.13))))\"\"\"\n",
    "sentence = \"\"\"Consensus on India will be difficult to reach when the NSG meets in November 2007 .\"\"\"\n",
    "amr = AMR.parse_string(amr_str)\n",
    "(new_amr, new_sentence, named_entities) = TokensReplacer.replace_named_entities(amr, sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Consensus on country will be difficult to reach when the organization meets in November 2007 .'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('c2',\n",
       "  'n',\n",
       "  [u'India'],\n",
       "  2,\n",
       "  2,\n",
       "  <amr_util.Node.Node instance at 0x7f4829a89710>),\n",
       " ('o',\n",
       "  'n2',\n",
       "  [u'NSG'],\n",
       "  10,\n",
       "  10,\n",
       "  <amr_util.Node.Node instance at 0x7f4829a878c0>)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "named_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(new_amr, new_sentence, date_entities) = TokensReplacer.replace_date_entities(new_amr, new_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Consensus on country will be difficult to reach when the organization meets in date-entity .'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For <span style=\"color:red\">date-entities</span> we store information concerning the concept, quantity pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('d2',\n",
       "  ['11', '2007'],\n",
       "  ['month', 'year'],\n",
       "  13,\n",
       "  14,\n",
       "  <amr_util.Node.Node instance at 0x7f482972aa28>)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mappings between node variables and their corresponding concepts.\n",
      "\n",
      "{'c': 'consensus', 'd': 'difficult', 'm': 'meet-03', 'o': 'organization', 'r': 'reach-01', 'c2': 'country', 'd2': 'date-entity'}\n",
      "\n",
      "Mappings between nodes and all the aligned tokens: If the nodes don't havea variable (polarity, literals, quantities, interrogatives), they specify both the aligned tokens and the parent in order to uniquely identify them\n",
      "\n",
      "{'c': ['0'], 'd': ['5'], 'm': ['11'], 'o': ['10'], 'r': ['7'], 'c2': ['2'], 'd2': [13]}\n",
      "\n",
      "Mappings between relations and tokens. Uniquely identified by also specifying the parent of that relation.\n",
      "\n",
      "{'topic': [[('1', 'c')]], 'domain': [[('4', 'd')]], 'time': [[('12', 'm')], [('8', 'r')]], 'month': [[('13', 'd2')]]}\n",
      "\n",
      "Mappings from a node to each child, along with the relation between them.\n",
      "\n",
      "Key: c\n",
      "topic -> c2\n",
      "\n",
      "Key: d\n",
      "domain -> r\n",
      "\n",
      "Key: m\n",
      "ARG0 -> o\n",
      "time -> d2\n",
      "\n",
      "Key: o\n",
      "Leaf\n",
      "\n",
      "Key: r\n",
      "ARG1 -> c\n",
      "time -> m\n",
      "\n",
      "Key: c2\n",
      "Leaf\n",
      "\n",
      "Key: d2\n",
      "Leaf\n",
      "\n",
      "\n",
      "All the nodes in the amr should appear here.\n",
      "\n",
      "['c', 'd', 'm', 'o', 'r', 'c2', 'd2']\n",
      "\n",
      "Creating custom AMR.\n",
      "\n",
      "\n",
      "Custom AMR token to concepts dict\n",
      "\n",
      "{0: ('c', 'consensus'), 2: ('c2', 'country'), 5: ('d', 'difficult'), 7: ('r', 'reach-01'), 10: ('o', 'organization'), 11: ('m', 'meet-03'), 13: ('d2', 'date-entity')}\n",
      "\n",
      "Custom AMR relations dict\n",
      "\n",
      "{('o', 'm'): ('ARG0', [], ['10']), ('c', 'r'): ('ARG1', ['c2'], ['0']), ('m', 'r'): ('time', ['o', 'd2'], ['11']), ('r', 'd'): ('domain', ['c', 'm'], ['7']), ('d', ''): ('', ['r'], ['5']), ('c2', 'c'): ('topic', [], ['2']), ('d2', 'm'): ('time', [], [13])}\n",
      "\n",
      "Custom AMR parent dict\n",
      "\n",
      "{'c': 'r', 'd': '', 'm': 'r', 'o': 'm', 'r': 'd', 'c2': 'c', 'd2': 'm'}\n",
      "['SH_consensus_c', 'DN', 'SH_country_c2', 'RR_topic', 'DN', 'DN', 'SH_difficult_d', 'DN', 'SH_reach-01_r', 'DN', 'DN', 'SH_organization_o', 'SH_meet-03_m', 'RL_ARG0', 'DN', 'SH_date-entity_d2', 'RR_time', 'RR_time', 'DN', 'SW', 'RL_ARG1', 'RR_domain']\n"
     ]
    }
   ],
   "source": [
    "custom_amr = generate_custom_amr(new_amr)\n",
    "actions = generate_action_sequence(custom_amr, new_sentence)\n",
    "print actions\n",
    "deps = extract_dependencies(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model path is:\n",
      "./models/all_epochs=15_maxlen=30_embeddingsdim=300\n",
      "Word index len: \n",
      "7107\n",
      "Test data shape: \n",
      "(1, 15)\n",
      "(1, 22)\n",
      "(1,)\n",
      "1\n",
      "Found 400000 word vectors.\n",
      "Embedding match for volume-quantity\n",
      "Embedding match for distance-quantity\n",
      "Embedding match for energy-quantity\n",
      "Embedding match for power-quantity\n",
      "Embedding match for mass-quantity\n",
      "Embedding match for monetary-quantity\n",
      "Embedding match for temporal-quantity\n",
      "Embedding match for date-entity\n",
      "First 2 not found: [\"don'cha\", 'it...']\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_19 (InputLayer)            (None, 30)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_20 (InputLayer)            (None, 30)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_21 (InputLayer)            (None, 30)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_22 (InputLayer)            (None, 30)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)          (None, 30, 300)       2132700                                      \n",
      "____________________________________________________________________________________________________\n",
      "input_23 (InputLayer)            (None, 30, 5)         0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_24 (InputLayer)            (None, 30, 6)         0                                            \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)      (None, 30, 1211)      0                                            \n",
      "____________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                    (None, 30, 1024)      9158656                                      \n",
      "____________________________________________________________________________________________________\n",
      "time_distributed_4 (TimeDistribu (None, 30, 5)         5125                                         \n",
      "====================================================================================================\n",
      "Total params: 11,296,481.0\n",
      "Trainable params: 9,163,781.0\n",
      "Non-trainable params: 2,132,700.0\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_19 (InputLayer)            (None, 30)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_20 (InputLayer)            (None, 30)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_21 (InputLayer)            (None, 30)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_22 (InputLayer)            (None, 30)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)          (None, 30, 300)       2132700                                      \n",
      "____________________________________________________________________________________________________\n",
      "input_23 (InputLayer)            (None, 30, 5)         0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_24 (InputLayer)            (None, 30, 6)         0                                            \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)      (None, 30, 1211)      0                                            \n",
      "____________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                    (None, 30, 1024)      9158656                                      \n",
      "____________________________________________________________________________________________________\n",
      "time_distributed_4 (TimeDistribu (None, 30, 5)         5125                                         \n",
      "====================================================================================================\n",
      "Total params: 11,296,481.0\n",
      "Trainable params: 9,163,781.0\n",
      "Non-trainable params: 2,132,700.0\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "Buffer and stack at end of prediction\n",
      "[]\n",
      "[2480]\n",
      "Sentence\n",
      "consensus on country will be difficult to reach when the organization meets in date-entity . \n",
      "\n",
      "Predicted\n",
      "SH DN SH DN DN SH RL DN DN SH DN SH SH RL RL RR DN SH RR DN RR \n",
      "\n",
      "Actual\n",
      "SH DN SH RR DN DN SH DN SH DN DN SH SH RL DN SH RR RR DN SW RL RR \n",
      "\n",
      "Predictions with old labels: \n",
      "['SH_consensus', 'DN', 'SH_country', 'DN', 'DN', 'SH_difficult', 'RL_topic', 'DN', 'DN', 'SH_reach-01', 'DN', 'SH_organization', 'SH_meet-03', 'RL_ARG0', 'RL_time', 'RR_time', 'DN', 'SH_date-entity', 'RR_ARG1', 'DN', 'RR_domain']\n",
      "Original Amr\n",
      "(d / difficult~e.5 \n",
      "      :domain~e.4 (r / reach-01~e.7 \n",
      "            :ARG1 (c / consensus~e.0 \n",
      "                  :topic~e.1 (c2 / country :wiki \"India\" \n",
      "                        :name (n / name :op1 \"India\"~e.2))) \n",
      "            :time~e.8 (m / meet-03~e.11 \n",
      "                  :ARG0 (o / organization :wiki \"Nuclear_Suppliers_Group\" \n",
      "                        :name (n2 / name :op1 \"NSG\"~e.10)) \n",
      "                  :time~e.12 (d2 / date-entity :year 2007~e.14 :month~e.13 11~e.13))))\n",
      "Predicted Amr\n",
      "( d1 / consensus \n",
      "\t:domain  ( d1_1 / difficult \n",
      "\t\t:topic  ( d1_1_1 / country )\n",
      "\t\t:time  ( d1_1_2 / meet-03 \n",
      "\t\t\t:ARG0  ( d1_1_2_1 / organization )\n",
      "\t\t\t:time  ( d1_1_2_2 / reach-01 )\n",
      "\t\t)\n",
      "\t\t:ARG1  ( d1_1_3 / date-entity )\n",
      "\t)\n",
      ")\n",
      "Smatch f-score 0.421053\n",
      "1/1 [==============================] - 0s\n"
     ]
    }
   ],
   "source": [
    "predictions = test(model_name=model2, tokenizer_path=\"./tokenizers/full_tokenizer.dump\", test_case_name=\"test4\",\n",
    "     data=[(new_sentence, actions, amr_str, deps, [], [])], max_len=max_len2, embedding_dim=embeddings_dim2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/example4_pred.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "named_entities = [(n[3], n[2]) for n in named_entities]\n",
    "date_entities = [(d[3], d[2], d[1]) for d in date_entities]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(13, ['month', 'year'], ['11', '2007'])]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions with old labels: \n",
      "['SH_consensus', 'DN', 'SH_country', 'DN', 'DN', 'SH_difficult', 'RL_topic', 'DN', 'DN', 'SH_reach-01', 'DN', 'SH_organization', 'SH_meet-03', 'RL_ARG0', 'RL_time', 'RR_time', 'DN', 'SH_date-entity', 'RR_ARG1', 'DN', 'RR_domain']\n",
      "Original Amr\n",
      "(d / difficult~e.5 \n",
      "      :domain~e.4 (r / reach-01~e.7 \n",
      "            :ARG1 (c / consensus~e.0 \n",
      "                  :topic~e.1 (c2 / country :wiki \"India\" \n",
      "                        :name (n / name :op1 \"India\"~e.2))) \n",
      "            :time~e.8 (m / meet-03~e.11 \n",
      "                  :ARG0 (o / organization :wiki \"Nuclear_Suppliers_Group\" \n",
      "                        :name (n2 / name :op1 \"NSG\"~e.10)) \n",
      "                  :time~e.12 (d2 / date-entity :year 2007~e.14 :month~e.13 11~e.13))))\n",
      "Predicted Amr\n",
      "( d1 / consensus \n",
      "\t:domain  ( d1_1 / difficult \n",
      "\t\t:topic  ( d1_1_1 / country \n",
      "\t\t\t:wiki \"India\"\n",
      "\t\t\t:name  ( d1_1_1_1 / name \n",
      "\t\t\t\t:op1 \"India\"\n",
      "\t\t\t)\n",
      "\t\t)\n",
      "\t\t:time  ( d1_1_2 / meet-03 \n",
      "\t\t\t:ARG0  ( d1_1_2_1 / organization \n",
      "\t\t\t\t:wiki \"NSG\"\n",
      "\t\t\t\t:name  ( d1_1_2_1_1 / name \n",
      "\t\t\t\t\t:op1 \"NSG\"\n",
      "\t\t\t\t)\n",
      "\t\t\t)\n",
      "\t\t\t:time  ( d1_1_2_2 / reach-01 )\n",
      "\t\t)\n",
      "\t\t:ARG1  ( d1_1_3 / date-entity \n",
      "\t\t\t:month \"11\"\n",
      "\t\t\t:year \"2007\"\n",
      "\t\t)\n",
      "\t)\n",
      ")\n",
      "Smatch f-score 0.708333\n"
     ]
    }
   ],
   "source": [
    "vocab_acts = support.Vocab.from_list(['SH', 'RL', 'RR', 'DN', 'SW'])\n",
    "action_objects = support.oracle_actions_to_action_index(actions, vocab_acts)\n",
    "action_indices = [a.index for a in action_objects]\n",
    "action_labels = [a.label for a in action_objects]\n",
    "\n",
    "act = asr.ActionConceptTransfer()\n",
    "act.load_from_action_indices_and_labels(action_indices, action_labels)\n",
    "pred_label = act.populate_new_actions(predictions[0])\n",
    "print 'Predictions with old labels: '\n",
    "print pred_label\n",
    "predicted_amr_str = asr.reconstruct_all_ne(pred_label, named_entities, date_entities)\n",
    "\n",
    "\n",
    "smatch_results = smatch_util.SmatchAccumulator()\n",
    "original_amr = smatch_amr.AMR.parse_AMR_line(amr_str)\n",
    "predicted_amr = smatch_amr.AMR.parse_AMR_line(predicted_amr_str)\n",
    "smatch_f_score = smatch_results.compute_and_add(predicted_amr, original_amr)\n",
    "\n",
    "print 'Original Amr'\n",
    "print amr_str\n",
    "print 'Predicted Amr'\n",
    "print predicted_amr_str\n",
    "print 'Smatch f-score %f' % smatch_f_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### We now make predictions for a full dataset using our best single LSTM model. This model was trained on all the corpuses, with a total of 8241 train sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/99 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resources/alignments/split/dev\n",
      "resources/alignments/split/dev/deft-p2-amr-r1-alignments-dev-xinhua.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 99/99 [00:09<00:00,  8.15it/s]\n",
      "CRITICAL:root:Failed: 81 out of 99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model path is:\n",
      "./models/all_epochs=15_maxlen=30_embeddingsdim=300\n",
      "Word index len: \n",
      "7107\n",
      "Test data shape: \n",
      "(18,)\n",
      "(18,)\n",
      "(18,)\n",
      "18\n",
      "Found 400000 word vectors.\n",
      "Embedding match for volume-quantity\n",
      "Embedding match for distance-quantity\n",
      "Embedding match for energy-quantity\n",
      "Embedding match for power-quantity\n",
      "Embedding match for mass-quantity\n",
      "Embedding match for monetary-quantity\n",
      "Embedding match for temporal-quantity\n",
      "Embedding match for date-entity\n",
      "First 2 not found: [\"don'cha\", 'it...']\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_25 (InputLayer)            (None, 30)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_26 (InputLayer)            (None, 30)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_27 (InputLayer)            (None, 30)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_28 (InputLayer)            (None, 30)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)          (None, 30, 300)       2132700                                      \n",
      "____________________________________________________________________________________________________\n",
      "input_29 (InputLayer)            (None, 30, 5)         0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_30 (InputLayer)            (None, 30, 6)         0                                            \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)      (None, 30, 1211)      0                                            \n",
      "____________________________________________________________________________________________________\n",
      "lstm_5 (LSTM)                    (None, 30, 1024)      9158656                                      \n",
      "____________________________________________________________________________________________________\n",
      "time_distributed_5 (TimeDistribu (None, 30, 5)         5125                                         \n",
      "====================================================================================================\n",
      "Total params: 11,296,481.0\n",
      "Trainable params: 9,163,781.0\n",
      "Non-trainable params: 2,132,700.0\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_25 (InputLayer)            (None, 30)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_26 (InputLayer)            (None, 30)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_27 (InputLayer)            (None, 30)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_28 (InputLayer)            (None, 30)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)          (None, 30, 300)       2132700                                      \n",
      "____________________________________________________________________________________________________\n",
      "input_29 (InputLayer)            (None, 30, 5)         0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_30 (InputLayer)            (None, 30, 6)         0                                            \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)      (None, 30, 1211)      0                                            \n",
      "____________________________________________________________________________________________________\n",
      "lstm_5 (LSTM)                    (None, 30, 1024)      9158656                                      \n",
      "____________________________________________________________________________________________________\n",
      "time_distributed_5 (TimeDistribu (None, 30, 5)         5125                                         \n",
      "====================================================================================================\n",
      "Total params: 11,296,481.0\n",
      "Trainable params: 9,163,781.0\n",
      "Non-trainable params: 2,132,700.0\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "Sentence\n",
      "( end ) \n",
      "\n",
      "Predicted\n",
      "\n",
      "\n",
      "Actual\n",
      "DN SH DN \n",
      "\n",
      "Buffer and stack at end of prediction\n",
      "[]\n",
      "[9]\n",
      "Sentence\n",
      "person expressed welcome at the periodic visit of person . \n",
      "\n",
      "Predicted\n",
      "SH SH SH RL DN DN SH SH DN SH RR RR RR DN RR \n",
      "\n",
      "Actual\n",
      "SH DN SH RL DN DN SH SH RL DN SH RR RR DN \n",
      "\n",
      "Predictions with old labels: \n",
      "['SH_person', 'SH_welcome-01', 'SH_periodic', 'RL_ARG0', 'DN', 'DN', 'SH_visit-01', 'SH_person', 'DN', 'SH_unk', 'RR_frequency', 'RR_ARG0', 'RR_ARG1', 'DN', 'RR_unk']\n",
      "Original Amr\n",
      "(w / welcome-01~e.3 \n",
      "      :ARG0 (p4 / person :wiki \"Li_Peng\" \n",
      "            :name (n / name :op1 \"Peng\"~e.0 :op2 \"Li\"~e.1)) \n",
      "      :ARG1~e.4 (v2 / visit-01~e.7 \n",
      "            :ARG0~e.8 (p5 / person :wiki \"Vasco_Joaquim_Rocha_Vieira\" \n",
      "                  :name (n2 / name :op1 \"Viera\"~e.9)) \n",
      "            :frequency (p / periodic~e.6)))\n",
      "\n",
      "Predicted Amr\n",
      "( d1 / person \n",
      "\t:wiki \"Peng_Li\"\n",
      "\t:name  ( d1_1 / name \n",
      "\t\t:op1 \"Peng\"\n",
      "\t\t:op2 \"Li\"\n",
      "\t)\n",
      "\t:unk  ( d1_2 / periodic \n",
      "\t\t:ARG0  ( d1_2_1 / welcome-01 )\n",
      "\t\t:ARG1  ( d1_2_2 / visit-01 \n",
      "\t\t\t:ARG0  ( d1_2_2_1 / person \n",
      "\t\t\t\t:frequency  ( d1_2_2_1_1 / unk )\n",
      "\t\t\t)\n",
      "\t\t)\n",
      "\t)\n",
      ")\n",
      "Smatch f-score 0.555556\n",
      "Buffer and stack at end of prediction\n",
      "[]\n",
      "[183]\n",
      "Sentence\n",
      "both sides should resolve the outstanding issues in the spirit of friendly consultations . \n",
      "\n",
      "Predicted\n",
      "SH SH SH SH RL DN SH SH RL DN DN SH RL DN SH SH RL RL RR RR DN RR \n",
      "\n",
      "Actual\n",
      "SH SH RL SH SH DN SH SH RL RR DN DN SH DN SH SH RL RR RR DN SW RL RR \n",
      "\n",
      "Predictions with old labels: \n",
      "['SH_both', 'SH_side', 'SH_recommend-01', 'SH_resolve-01', 'RL_mod', 'DN', 'SH_outstanding', 'SH_issue-02', 'RL_mod', 'DN', 'DN', 'SH_spirit', 'RL_ARG1', 'DN', 'SH_friendly-01', 'SH_consult-01', 'RL_ARG1-of', 'RL_mod', 'RR_manner', 'RR_ARG0', 'DN', 'RR_ARG1']\n",
      "Original Amr\n",
      "(r / recommend-01~e.2 \n",
      "      :ARG1 (r2 / resolve-01~e.3 \n",
      "            :ARG0 (s / side~e.1 \n",
      "                  :mod (b / both~e.0)) \n",
      "            :ARG1 (i / issue-02~e.6 \n",
      "                  :mod (o / outstanding~e.5)) \n",
      "            :manner~e.7 (s2 / spirit~e.9 \n",
      "                  :mod~e.10 (c2 / consult-01~e.12 \n",
      "                        :ARG1-of (f / friendly-01~e.11)))))\n",
      "\n",
      "Predicted Amr\n",
      "( d1 / both \n",
      "\t:ARG1  ( d1_1 / side \n",
      "\t\t:ARG0  ( d1_1_1 / resolve-01 \n",
      "\t\t\t:mod  ( d1_1_1_1 / recommend-01 )\n",
      "\t\t\t:manner  ( d1_1_1_2 / consult-01 \n",
      "\t\t\t\t:ARG1-of  ( d1_1_1_2_1 / friendly-01 )\n",
      "\t\t\t\t:mod  ( d1_1_1_2_2 / spirit \n",
      "\t\t\t\t\t:ARG1  ( d1_1_1_2_2_1 / issue-02 \n",
      "\t\t\t\t\t\t:mod  ( d1_1_1_2_2_1_1 / outstanding )\n",
      "\t\t\t\t\t)\n",
      "\t\t\t\t)\n",
      "\t\t\t)\n",
      "\t\t)\n",
      "\t)\n",
      ")\n",
      "Smatch f-score 0.611111\n",
      "Sentence\n",
      "( end ) \n",
      "\n",
      "Predicted\n",
      "\n",
      "\n",
      "Actual\n",
      "DN SH DN \n",
      "\n",
      "Buffer and stack at end of prediction\n",
      "[]\n",
      "[47]\n",
      "Sentence\n",
      "but , as an auxiliary method of military measures , restarting a dialogue is also necessary . \n",
      "\n",
      "Predicted\n",
      "SH DN DN DN SH SH DN SH SH RL RL DN SH DN SH DN SH SH RL RR DN RR RR RR RR \n",
      "\n",
      "Actual\n",
      "SH DN DN DN SH SH RL DN SH SH RL RR DN SH DN SH RR DN SH SH RL RL RL RR DN \n",
      "\n",
      "Predictions with old labels: \n",
      "['SH_contrast-01', 'DN', 'DN', 'DN', 'SH_auxiliary', 'SH_method', 'DN', 'SH_military', 'SH_measure-02', 'RL_mod', 'RL_ARG0', 'DN', 'SH_restart-01', 'DN', 'SH_dialogue-01', 'DN', 'SH_also', 'SH_need-01', 'RL_instrument-of', 'RR_ARG1', 'DN', 'RR_mod', 'RR_ARG1', 'RR_prep-as', 'RR_ARG2']\n",
      "Original Amr\n",
      "(c / contrast-01~e.0 \n",
      "      :ARG2 (n / need-01~e.15 \n",
      "            :ARG1 (r / restart-01~e.10 \n",
      "                  :ARG1 (d / dialogue-01~e.12)) \n",
      "            :mod (a2 / also~e.14) \n",
      "            :prep-as~e.2 (m / method~e.5 \n",
      "                  :mod (a3 / auxiliary~e.4) \n",
      "                  :instrument-of~e.6 (m2 / measure-02~e.8 \n",
      "                        :ARG0 (m3 / military~e.7)))))\n",
      "\n",
      "Predicted Amr\n",
      "( d1 / contrast-01 \n",
      "\t:ARG2  ( d1_1 / auxiliary \n",
      "\t\t:prep-as  ( d1_1_1 / measure-02 \n",
      "\t\t\t:mod  ( d1_1_1_1 / military )\n",
      "\t\t\t:ARG0  ( d1_1_1_2 / method )\n",
      "\t\t\t:ARG1  ( d1_1_1_3 / restart-01 \n",
      "\t\t\t\t:mod  ( d1_1_1_3_1 / dialogue-01 \n",
      "\t\t\t\t\t:ARG1  ( d1_1_1_3_1_1 / need-01 \n",
      "\t\t\t\t\t\t:instrument-of  ( d1_1_1_3_1_1_1 / also )\n",
      "\t\t\t\t\t)\n",
      "\t\t\t\t)\n",
      "\t\t\t)\n",
      "\t\t)\n",
      "\t)\n",
      ")\n",
      "Smatch f-score 0.555556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence\n",
      "( end ) \n",
      "\n",
      "Predicted\n",
      "\n",
      "\n",
      "Actual\n",
      "DN SH DN \n",
      "\n",
      "Buffer and stack at end of prediction\n",
      "[]\n",
      "[372]\n",
      "Sentence\n",
      "currently , technology exports that country can provide reach 52 . \n",
      "\n",
      "Predicted\n",
      "DN DN SH SH SH SH RL SH SH RL RL SH RR RR DN DN RR \n",
      "\n",
      "Actual\n",
      "SH DN SH SH RL DN SH SH SH RL RL RR SH RL RL DN DN \n",
      "\n",
      "Predictions with old labels: \n",
      "['DN', 'DN', 'SH_current', 'SH_technology', 'SH_export-01', 'SH_country', 'RL_ARG1', 'SH_possible-01', 'SH_provide-01', 'RL_ARG1-of', 'RL_ARG0', 'SH_reach-01', 'RR_ARG1-of', 'RR_ARG0', 'DN', 'DN', 'RR_time']\n",
      "Original Amr\n",
      "(r / reach-01~e.8 :ARG1 52~e.9 \n",
      "      :ARG0 (e / export-01~e.3 \n",
      "            :ARG1 (t / technology~e.2) \n",
      "            :ARG1-of (p / provide-01~e.7 \n",
      "                  :ARG0 (c3 / country :wiki \"China\" \n",
      "                        :name (n / name :op1 \"China\"~e.5)) \n",
      "                  :ARG1-of (p2 / possible-01~e.6))) \n",
      "      :time (c4 / current~e.0))\n",
      "\n",
      "Predicted Amr\n",
      "( d1 / current \n",
      "\t:time  ( d1_1 / technology \n",
      "\t\t:ARG0  ( d1_1_1 / provide-01 \n",
      "\t\t\t:ARG1-of  ( d1_1_1_1 / possible-01 )\n",
      "\t\t\t:ARG0  ( d1_1_1_2 / country \n",
      "\t\t\t\t:wiki \"China\"\n",
      "\t\t\t\t:name  ( d1_1_1_2_1 / name \n",
      "\t\t\t\t\t:op1 \"China\"\n",
      "\t\t\t\t)\n",
      "\t\t\t\t:ARG1  ( d1_1_1_2_2 / export-01 )\n",
      "\t\t\t)\n",
      "\t\t\t:ARG1-of  ( d1_1_1_3 / reach-01 )\n",
      "\t\t)\n",
      "\t)\n",
      ")\n",
      "Smatch f-score 0.702703\n",
      "Sentence\n",
      "( end ) \n",
      "\n",
      "Predicted\n",
      "\n",
      "\n",
      "Actual\n",
      "DN SH DN \n",
      "\n",
      "Sentence\n",
      "( end ) \n",
      "\n",
      "Predicted\n",
      "\n",
      "\n",
      "Actual\n",
      "DN SH DN \n",
      "\n",
      "Buffer and stack at end of prediction\n",
      "[]\n",
      "[6279, 1791]\n",
      "Sentence\n",
      "the negotiations carried out thorough discussions on some principle and detailed issues and some understanding was reached . \n",
      "\n",
      "Predicted\n",
      "DN SH SH DN SH RL SH DN SH SH RL RL SH SH RL SH RL RL SH SH SH RL DN SH RR RR RR RR DN \n",
      "\n",
      "Actual\n",
      "DN SH DN DN SH SH RL RL DN SH SH SH RL RL SH SH RL RR RR SH RL SH SH RL DN SH RL RR DN \n",
      "\n",
      "Predictions with old labels: \n",
      "['DN', 'SH_negotiate-01', 'SH_thorough', 'DN', 'SH_discuss-01', 'RL_manner', 'SH_some', 'DN', 'SH_principle', 'SH_and', 'RL_ARG0', 'RL_op1', 'SH_detail-01', 'SH_issue-02', 'RL_quant', 'SH_and', 'RL_ARG1-of', 'RL_op2', 'SH_some', 'SH_understand-01', 'SH_reach-01', 'RL_ARG1', 'DN', 'SH_unk', 'RR_op1', 'RR_quant', 'RR_ARG1', 'RR_op2', 'DN']\n",
      "Original Amr\n",
      "(a / and~e.12 \n",
      "      :op1 (d / discuss-01~e.5 \n",
      "            :ARG0 (n / negotiate-01~e.1) \n",
      "            :ARG1~e.6 (a2 / and~e.9,12 \n",
      "                  :op1 (p / principle~e.8) \n",
      "                  :op2 (i / issue-02~e.11 \n",
      "                        :ARG1-of (d2 / detail-01~e.10)) \n",
      "                  :quant (s / some~e.7)) \n",
      "            :manner (t / thorough~e.4)) \n",
      "      :op2 (r / reach-01~e.16 \n",
      "            :ARG1 (u / understand-01~e.14 \n",
      "                  :quant (s2 / some~e.13))))\n",
      "\n",
      "Predicted Amr\n",
      "( d1 / discuss-01 \n",
      "\t:manner  ( d1_1 / thorough )\n",
      "\t:op2  ( d1_2 / and \n",
      "\t\t:ARG1-of  ( d1_2_1 / issue-02 \n",
      "\t\t\t:quant  ( d1_2_1_1 / detail-01 )\n",
      "\t\t)\n",
      "\t\t:op2  ( d1_2_2 / and \n",
      "\t\t\t:ARG0  ( d1_2_2_1 / principle )\n",
      "\t\t\t:op1  ( d1_2_2_2 / some )\n",
      "\t\t)\n",
      "\t\t:ARG1  ( d1_2_3 / some \n",
      "\t\t\t:quant  ( d1_2_3_1 / reach-01 \n",
      "\t\t\t\t:ARG1  ( d1_2_3_1_1 / understand-01 )\n",
      "\t\t\t\t:op1  ( d1_2_3_1_2 / unk )\n",
      "\t\t\t)\n",
      "\t\t)\n",
      "\t)\n",
      ")\n",
      "Smatch f-score 0.541667\n",
      "Buffer and stack at end of prediction\n",
      "[]\n",
      "[183]\n",
      "Sentence\n",
      "both sides unanimously believed that the talk had positive results . \n",
      "\n",
      "Predicted\n",
      "SH SH SH RL SH DN DN SH SH SH RL SH RL RL RR DN RR RR \n",
      "\n",
      "Actual\n",
      "SH SH RL SH SH RL RL DN DN SH DN SH SH RL RL RR DN \n",
      "\n",
      "Predictions with old labels: \n",
      "['SH_both', 'SH_side', 'SH_unanimous', 'RL_mod', 'SH_believe-01', 'DN', 'DN', 'SH_talk', 'SH_positive', 'SH_result-01', 'RL_manner', 'SH_unk', 'RL_ARG0', 'RL_ARG2', 'RR_ARG1', 'DN', 'RR_ARG1', 'RR_unk']\n",
      "Original Amr\n",
      "(b / believe-01~e.3 \n",
      "      :ARG0 (s / side~e.1 \n",
      "            :mod (b2 / both~e.0)) \n",
      "      :ARG1~e.4 (r / result-01~e.9 \n",
      "            :ARG1 (t / talk~e.6) \n",
      "            :ARG2 (p / positive~e.8)) \n",
      "      :manner~e.2 (u / unanimous~e.2))\n",
      "\n",
      "Predicted Amr\n",
      "( d1 / both \n",
      "\t:unk  ( d1_1 / unanimous \n",
      "\t\t:mod  ( d1_1_1 / side )\n",
      "\t\t:ARG1  ( d1_1_2 / believe-01 \n",
      "\t\t\t:ARG1  ( d1_1_2_1 / unk \n",
      "\t\t\t\t:ARG0  ( d1_1_2_1_1 / result-01 \n",
      "\t\t\t\t\t:manner  ( d1_1_2_1_1_1 / positive )\n",
      "\t\t\t\t)\n",
      "\t\t\t\t:ARG2  ( d1_1_2_1_2 / talk )\n",
      "\t\t\t)\n",
      "\t\t)\n",
      "\t)\n",
      ")\n",
      "Smatch f-score 0.466667\n",
      "Sentence\n",
      "( end ) \n",
      "\n",
      "Predicted\n",
      "\n",
      "\n",
      "Actual\n",
      "DN SH DN \n",
      "\n",
      "Buffer and stack at end of prediction\n",
      "[]\n",
      "[452]\n",
      "Sentence\n",
      "the talk was held in a practical and friendly atmosphere . \n",
      "\n",
      "Predicted\n",
      "DN SH DN SH DN DN SH SH RL SH RL RL DN DN RR \n",
      "\n",
      "Actual\n",
      "DN SH DN SH RL DN DN SH DN SH SH RL RL RR DN \n",
      "\n",
      "Predictions with old labels: \n",
      "['DN', 'SH_talk-01', 'DN', 'SH_hold-04', 'DN', 'DN', 'SH_practical', 'SH_friendly-01', 'RL_ARG1', 'SH_atmosphere', 'RL_ARG1-of', 'RL_mod', 'DN', 'DN', 'RR_location']\n",
      "Original Amr\n",
      "(h / hold-04~e.3 \n",
      "      :ARG1 (t / talk-01~e.1) \n",
      "      :location~e.4 (a / atmosphere~e.9 \n",
      "            :mod (p / practical~e.6) \n",
      "            :ARG1-of (f / friendly-01~e.8)))\n",
      "\n",
      "Predicted Amr\n",
      "( d1 / talk-01 \n",
      "\t:location  ( d1_1 / atmosphere \n",
      "\t\t:ARG1-of  ( d1_1_1 / friendly-01 \n",
      "\t\t\t:ARG1  ( d1_1_1_1 / practical )\n",
      "\t\t)\n",
      "\t\t:mod  ( d1_1_2 / hold-04 )\n",
      "\t)\n",
      ")\n",
      "Smatch f-score 0.600000\n",
      "Buffer and stack at end of prediction\n",
      "[]\n",
      "[9]\n",
      "Sentence\n",
      "person also made a presentation to person about country 's economic situation . \n",
      "\n",
      "Predicted\n",
      "SH DN SH DN SH RL DN SH DN SH RL DN SH SH RL RR RR DN RR \n",
      "\n",
      "Actual\n",
      "SH SH DN DN SH RL RL DN SH RR DN SH DN SH SH RL RL RR DN \n",
      "\n",
      "Predictions with old labels: \n",
      "['SH_person', 'DN', 'SH_also', 'DN', 'SH_present-01', 'RL_mod', 'DN', 'SH_person', 'DN', 'SH_country', 'RL_ARG0', 'DN', 'SH_economy', 'SH_situation', 'RL_ARG2', 'RR_mod', 'RR_poss', 'DN', 'RR_ARG1']\n",
      "Original Amr\n",
      "(p4 / present-01~e.4 \n",
      "      :ARG0 (p5 / person :wiki - \n",
      "            :name (n / name :op1 \"Zumegloph\"~e.0)) \n",
      "      :ARG1~e.8 (s / situation~e.12 \n",
      "            :mod (e / economy~e.11) \n",
      "            :poss~e.10 (c / country :wiki \"Kyrgyzstan\" \n",
      "                  :name (n3 / name :op1 \"Kirghizia\"~e.9))) \n",
      "      :ARG2~e.5 (p6 / person :wiki \"Li_Peng\" \n",
      "            :name (n2 / name :op1 \"Peng\"~e.6 :op2 \"Li\"~e.7)) \n",
      "      :mod (a / also~e.1))\n",
      "\n",
      "Predicted Amr\n",
      "( d1 / person \n",
      "\t:wiki \"Zumegloph\"\n",
      "\t:name  ( d1_1 / name \n",
      "\t\t:op1 \"Zumegloph\"\n",
      "\t)\n",
      "\t:ARG1  ( d1_2 / present-01 \n",
      "\t\t:mod  ( d1_2_1 / also )\n",
      "\t\t:poss  ( d1_2_2 / country \n",
      "\t\t\t:ARG0  ( d1_2_2_1 / person \n",
      "\t\t\t\t:wiki \"Peng_Li\"\n",
      "\t\t\t\t:name  ( d1_2_2_1_1 / name \n",
      "\t\t\t\t\t:op1 \"Peng\"\n",
      "\t\t\t\t\t:op2 \"Li\"\n",
      "\t\t\t\t)\n",
      "\t\t\t)\n",
      "\t\t\t:mod  ( d1_2_2_2 / situation \n",
      "\t\t\t\t:ARG2  ( d1_2_2_2_1 / economy )\n",
      "\t\t\t)\n",
      "\t\t)\n",
      "\t)\n",
      ")\n",
      "Smatch f-score 0.600000\n",
      "Sentence\n",
      "( end ) \n",
      "\n",
      "Predicted\n",
      "\n",
      "\n",
      "Actual\n",
      "DN SH DN \n",
      "\n",
      "Sentence\n",
      "( end ) \n",
      "\n",
      "Predicted\n",
      "\n",
      "\n",
      "Actual\n",
      "DN SH DN \n",
      "\n",
      "Buffer and stack at end of prediction\n",
      "[]\n",
      "[157]\n",
      "Sentence\n",
      "after the disintegration of the former country , these troop clusters were transferred to country ownership . \n",
      "\n",
      "Predicted\n",
      "SH DN SH DN DN DN SH DN SH RL SH RL SH DN SH DN SH SH RL RR DN RR RR RR RR \n",
      "\n",
      "Actual\n",
      "SH DN SH DN DN SH SH RL RR RR DN SH SH SH RL RL DN SH RL RL DN SH SH RL RR DN \n",
      "\n",
      "Predictions with old labels: \n",
      "['SH_after', 'DN', 'SH_disintegrate-01', 'DN', 'DN', 'DN', 'SH_former', 'DN', 'SH_country', 'RL_time', 'SH_this', 'RL_ARG1', 'SH_troop', 'DN', 'SH_cluster', 'DN', 'SH_transfer-01', 'SH_country', 'RL_op1', 'RR_consist-of', 'DN', 'RR_mod', 'RR_ARG1', 'RR_time', 'RR_ARG0']\n",
      "Original Amr\n",
      "(t / transfer-01~e.13 \n",
      "      :ARG1 (c / cluster~e.11 \n",
      "            :consist-of (t2 / troop~e.10) \n",
      "            :mod (t3 / this~e.9)) \n",
      "      :ARG2~e.14 (o / own-01~e.16 \n",
      "            :ARG0 (c4 / country :wiki \"Russia\" \n",
      "                  :name (n / name :op1 \"Russia\"~e.15))) \n",
      "      :time (a / after~e.0 \n",
      "            :op1 (d / disintegrate-01~e.2 \n",
      "                  :ARG1~e.3 (c5 / country :wiki \"Soviet_Union\" \n",
      "                        :name (n2 / name :op1 \"Soviet\"~e.6 :op2 \"Union\"~e.7) \n",
      "                        :time (f / former~e.5)))))\n",
      "\n",
      "Predicted Amr\n",
      "( d1 / after \n",
      "\t:ARG0  ( d1_1 / disintegrate-01 \n",
      "\t\t:time  ( d1_1_1 / this \n",
      "\t\t\t:ARG1  ( d1_1_1_1 / country \n",
      "\t\t\t\t:time  ( d1_1_1_1_1 / former \n",
      "\t\t\t\t\t:wiki \"Soviet_Union\"\n",
      "\t\t\t\t\t:name  ( d1_1_1_1_1_1 / name \n",
      "\t\t\t\t\t\t:op1 \"Soviet\"\n",
      "\t\t\t\t\t\t:op2 \"Union\"\n",
      "\t\t\t\t\t)\n",
      "\t\t\t\t)\n",
      "\t\t\t)\n",
      "\t\t\t:ARG1  ( d1_1_1_2 / troop \n",
      "\t\t\t\t:mod  ( d1_1_1_2_1 / cluster \n",
      "\t\t\t\t\t:consist-of  ( d1_1_1_2_1_1 / country \n",
      "\t\t\t\t\t\t:wiki \"Russia\"\n",
      "\t\t\t\t\t\t:name  ( d1_1_1_2_1_1_1 / name \n",
      "\t\t\t\t\t\t\t:op1 \"Russia\"\n",
      "\t\t\t\t\t\t)\n",
      "\t\t\t\t\t\t:op1  ( d1_1_1_2_1_1_2 / transfer-01 )\n",
      "\t\t\t\t\t)\n",
      "\t\t\t\t)\n",
      "\t\t\t)\n",
      "\t\t)\n",
      "\t)\n",
      ")\n",
      "Smatch f-score 0.607143\n",
      "Sentence\n",
      "( end ) \n",
      "\n",
      "Predicted\n",
      "\n",
      "\n",
      "Actual\n",
      "DN SH DN \n",
      "\n",
      "18/18 [==============================] - 0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[],\n",
       " [0, 0, 0, 1, 3, 3, 0, 0, 3, 0, 2, 2, 2, 3, 2],\n",
       " [0, 0, 0, 0, 1, 3, 0, 0, 1, 3, 3, 0, 1, 3, 0, 0, 1, 1, 2, 2, 3, 2],\n",
       " [],\n",
       " [0, 3, 3, 3, 0, 0, 3, 0, 0, 1, 1, 3, 0, 3, 0, 3, 0, 0, 1, 2, 3, 2, 2, 2, 2],\n",
       " [],\n",
       " [3, 3, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 2, 2, 3, 3, 2],\n",
       " [],\n",
       " [],\n",
       " [3,\n",
       "  0,\n",
       "  0,\n",
       "  3,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  3,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  3,\n",
       "  0,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  3],\n",
       " [0, 0, 0, 1, 0, 3, 3, 0, 0, 0, 1, 0, 1, 1, 2, 3, 2, 2],\n",
       " [],\n",
       " [3, 0, 3, 0, 3, 3, 0, 0, 1, 0, 1, 1, 3, 3, 2],\n",
       " [0, 3, 0, 3, 0, 1, 3, 0, 3, 0, 1, 3, 0, 0, 1, 2, 2, 3, 2],\n",
       " [],\n",
       " [],\n",
       " [0, 3, 0, 3, 3, 3, 0, 3, 0, 1, 0, 1, 0, 3, 0, 3, 0, 0, 1, 2, 3, 2, 2, 2, 2],\n",
       " []]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_file(model2, tokenizer_path=\"./tokenizers/full_tokenizer.dump\",\n",
    "            test_case_name= \"test5\",\n",
    "            test_data_path=\"deft-p2-amr-r1-alignments-dev-xinhua.txt\", max_len=max_len2,\n",
    "            embedding_dim=embeddings_dim2, test_source=\"dev\", with_reattach=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
